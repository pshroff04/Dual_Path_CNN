{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prate\\AppData\\Local\\conda\\conda\\envs\\mlenv\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x201402f60b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import os.path\n",
    "\n",
    "torch.random.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "image_path = '../Dataset/MSCOCO/image/'\n",
    "train_json = '../Dataset/MSCOCO/annotations_trainval2014/annotations/captions_train2014.json'\n",
    "test_json = '../Dataset/MSCOCO/annotations_trainval2014/annotations/captions_val2014.json'\n",
    "word2VecBin_path = '../../../Downloads/GoogleNews-vectors-negative300.bin'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "isgpu = torch.cuda.is_available()\n",
    "if isgpu:\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "#Tensorboard writer\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "\n",
    "def create_dataset_pd(train_json, test_json):\n",
    "    train_dataset = json.load(open(train_json, 'r'))\n",
    "    test_dataset = json.load(open(test_json, 'r'))\n",
    "\n",
    "    td_img = pd.DataFrame(test_dataset['images'])\n",
    "    td_ann = pd.DataFrame(test_dataset['annotations'])\n",
    "    test_p = pd.merge(td_img, td_ann, left_on='id', right_on='image_id')\n",
    "    test_p.drop_duplicates('image_id', inplace=True)\n",
    "\n",
    "    train_img = pd.DataFrame(train_dataset['images'])\n",
    "    train_ann = pd.DataFrame(train_dataset['annotations'])\n",
    "    train_p = pd.merge(train_img, train_ann, left_on='id', right_on='image_id')\n",
    "    train_p.drop_duplicates('image_id', inplace=True)\n",
    "    dataset = train_p.append(test_p, ignore_index=True)\n",
    "    dataset.drop(columns=['coco_url', 'date_captured',  'flickr_url', 'height',\n",
    "           'id_x', 'license', 'width', 'id_y'], inplace=True)\n",
    "    return dataset\n",
    "\n",
    "dataset = create_dataset_pd(train_json, test_json)\n",
    "\n",
    "pd_train_dataset = dataset.iloc[:-10000,:]\n",
    "pd_val_dataset = dataset.iloc[-10000:-5000,:]\n",
    "pd_test_dataset = dataset.iloc[-5000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pd_train_dataset.shape[0] == 113287"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing        \n",
    "word2Vec = gensim.models.KeyedVectors.load_word2vec_format(word2VecBin_path, binary=True)\n",
    "\n",
    "#Remove this \n",
    "train_dataset = json.load(open(train_json, 'r'))\n",
    "test_dataset = json.load(open(test_json, 'r'))\n",
    "\n",
    "def preprocess(x):\n",
    "    #lower\n",
    "    x = x.lower()\n",
    "    #x = re.sub(r'\\d+','', x) #remove numbers\n",
    "    x = x.translate(str.maketrans('', '', string.punctuation))#remove punctuation\n",
    "    x = x.strip() #whitespace\n",
    "    return word_tokenize(x)\n",
    "#idx zero - reverse for zero padding\n",
    "idx=1\n",
    "vocab = set()\n",
    "word2idx= {}\n",
    "for dataset in (train_dataset, test_dataset):\n",
    "    for entry in dataset['annotations']:\n",
    "        text = entry['caption']\n",
    "        for word in preprocess(text):\n",
    "            if  word not in word2idx and word in word2Vec:\n",
    "                word2idx[word] = idx\n",
    "                vocab.add(word)\n",
    "                idx +=1\n",
    "                #print(min(list(word2idx.values())))\n",
    "\n",
    "weight_matrix = torch.zeros((len(vocab)+1, 300)) # weight matrix's first entry will be for zero index\n",
    "for word in word2idx.keys():\n",
    "    weight_matrix[word2idx[word]] = torch.from_numpy(word2Vec[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image Data preprocessing\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_path, dataset, imgSize, transforms=None):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.imgSize = imgSize\n",
    "        self.transforms = transforms\n",
    "        self.image_path = image_path\n",
    "#         image = pd.DataFrame(dataset['images'])\n",
    "#         ann = pd.DataFrame(dataset['annotations'])\n",
    "        \n",
    "#         self.data = pd.merge(left=image, right=ann, left_on='id', right_on='image_id')\n",
    "#         self.data.drop(columns=['coco_url', 'date_captured',  'flickr_url', 'height',\n",
    "#            'id_x', 'license', 'width', 'id_y'], inplace=True)\n",
    "#         self.data.drop_duplicates('image_id', inplace=True)\n",
    "#         self.data.index = range(len(self.data))\n",
    "        self.data = dataset\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        #Get Image\n",
    "        path = self.image_path + 'train2014/' + self.data.loc[index]['file_name']\n",
    "        if (not os.path.isfile(path)):\n",
    "            path = self.image_path + 'val2014/' + self.data.loc[index]['file_name']\n",
    "        img = cv2.imread(path)\n",
    "        img_w, img_h = img.shape[1], img.shape[0]\n",
    "        w, h = 256,256\n",
    "        new_w = round(img_w * min(w/img_w, h/img_h))\n",
    "        new_h = round(img_h * min(w/img_w, h/img_h))\n",
    "        resized_image = cv2.resize(img, (new_w,new_h), interpolation = cv2.INTER_CUBIC)\n",
    "        if transforms is not None:\n",
    "            img = self.transforms(Image.fromarray(resized_image)) #Input shoudl be PIL Image\n",
    "        #img = cv2.resize(resized_image, self.imgSize)\n",
    "        img = np.asarray(img)\n",
    "        img = img[:,:,::-1].transpose((2,0,1)).copy()        \n",
    "        img = torch.from_numpy(img).float().div(255.0)\n",
    "        \n",
    "        #Get textIndexs for embedding layer\n",
    "        caption = self.data.loc[index]['caption']\n",
    "        words = preprocess(caption)[:32]\n",
    "        txtIndex = [0 for _ in range(32)] #32- paper \n",
    "        for w in range(len(words)):\n",
    "            word = words[w]\n",
    "            if word in vocab:\n",
    "                txtIndex[w]  = word2idx[word]\n",
    "        wordIndex = torch.LongTensor(txtIndex)   \n",
    "        \n",
    "        return img, wordIndex, index         #label = index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "#transformation applied\n",
    "transformations = transforms.Compose([#transforms.Resize((224,224), interpolation=3),\n",
    "                                       transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                       transforms.RandomResizedCrop(224,interpolation=3),\n",
    "                                       ])\n",
    "\n",
    "train_ds = CustomDataset(image_path, pd_train_dataset, (224,224), transformations)\n",
    "trainLoader = DataLoader(train_ds, shuffle=True, batch_size=4)\n",
    "\n",
    "# eval_ds = CustomDataset(dataset, (224,224), transformations)\n",
    "# evalLoader = DataLoader(eval_ds, shuffle=True, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "class ImageCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, stageI=True):\n",
    "        super(ImageCNN, self).__init__()\n",
    "        re = resnet50(pretrained=True)\n",
    "        #remove the last classification layer\n",
    "        self.resnet = nn.Sequential(*list(re.children())[:-1])\n",
    "#         if isgpu:\n",
    "#             self.resnet.to('cuda')\n",
    "        #During StageI training -> resnet weights are fixed with pretrained weights\n",
    "        if stageI: \n",
    "            for weights in self.resnet.parameters():\n",
    "                weights.requires_grad_(False)\n",
    "        \n",
    "        self.fc = nn.Linear(2048, 2048)\n",
    "        self.bn = nn.BatchNorm1d(2048)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropOut = nn.Dropout(0.8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x) # (N,2048,1,1)\n",
    "        x = torch.flatten(x,1) # (N,2048)\n",
    "        x = self.relu(self.bn(self.fc(x)))\n",
    "        x = self.relu(self.bn(self.fc(x))) # As per their MATLAB implementation\n",
    "        return self.dropOut(x)\n",
    "\n",
    "##Test Model -> Input (N,3,224,224) and Output (N,2048)\n",
    "# net1 = ImageCNN()\n",
    "# x1= torch.rand((2,3,224,224))\n",
    "# y1 = net1(x1)\n",
    "# print(y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#textCNN -> The input should be the output of word2vec (N,300,32,1)\n",
    "class BasicBlockText(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channel, intermediate_channel):\n",
    "        super(BasicBlockText, self).__init__()\n",
    "        self.bbConv1 = nn.Conv2d(input_channel, intermediate_channel, \n",
    "                                 kernel_size=(1,1), stride=(1,1), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.bbBatchNorm1 = nn.BatchNorm2d(intermediate_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.bbConv2 = nn.Conv2d(intermediate_channel, intermediate_channel, \n",
    "                                 kernel_size=(1,2), stride=(1,1), \n",
    "                                 padding=(0,1),bias=False, dilation = (1,2))\n",
    "        self.bbBatchNorm2 = nn.BatchNorm2d(intermediate_channel)\n",
    "        \n",
    "        self.bbConv3 = nn.Conv2d(intermediate_channel, input_channel, kernel_size=(1,1), stride=(1,1), padding=(0,0), bias=False)\n",
    "        self.bbBatchNorm3 = nn.BatchNorm2d(input_channel)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity  = x\n",
    "        out = self.relu(self.bbBatchNorm1(self.bbConv1(x)))\n",
    "        out = self.relu(self.bbBatchNorm2(self.bbConv2(out)))\n",
    "        out = self.bbBatchNorm3(self.bbConv3(out))\n",
    "        \n",
    "        out += identity\n",
    "        return self.relu(out)\n",
    "\n",
    "        \n",
    "class textCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channel): #300\n",
    "        super(textCNN, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        #self.word2Vec = nn.Embedding.from_pretrained(self.__load_word2vec(path))\n",
    "        \n",
    "        self.emb_layer,_,_ = self.create_emb_layer(weight_matrix)\n",
    "        #--------First CNN block----------------\n",
    "        self.b1Conv1_0 = nn.Conv2d(input_channel, 128, \n",
    "                                 kernel_size=(1,1), stride=(1,1), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b1bn1_0 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.b1Conv2 = nn.Conv2d(128, 128, \n",
    "                                 kernel_size=(1,2), stride=(1,1), \n",
    "                                 padding=(0,1), bias=False, dilation=(1,2))\n",
    "        self.b1bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.b1Conv3 = nn.Conv2d(128, 256, \n",
    "                                 kernel_size=(1,1), stride=(1,1), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b1bn3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        #input here too\n",
    "        self.b1Conv1_1 = nn.Conv2d(input_channel, 256,\n",
    "                               kernel_size=(1,1), stride=(1,1), \n",
    "                               padding=(0,0), bias=False)\n",
    "        self.b1bn1_1 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        #Adding first basicblock (in matlab code i=2:3)\n",
    "        self.layer1  = self.__make_layer(input_channel=256, intermediate_channel=64, \n",
    "                                     num_blocks=2)\n",
    "        \n",
    "        #--------Second CNN block----------------\n",
    "        self.b2Conv1_0 = nn.Conv2d(256, 512, \n",
    "                                 kernel_size=(1,1), stride=(1,1), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b2bn1_0 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.b2Conv2 = nn.Conv2d(512, 512, \n",
    "                                 kernel_size=(1,2), stride=(2,2), \n",
    "                                 padding=(0,1), bias=False, dilation= (1,2))\n",
    "        self.b2bn2 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.b2Conv3 = nn.Conv2d(512, 512, \n",
    "                                 kernel_size=(1,1), stride=(1,1), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b2bn3 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.b2Conv1_1 = nn.Conv2d(256, 512, \n",
    "                                 kernel_size=(1,1), stride=(2,2), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b2bn1_1 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        #Add second basicblock (in matlb i = 2:4)\n",
    "        self.layer2 =  self.__make_layer(input_channel=512, intermediate_channel=128, \n",
    "                                     num_blocks=3)\n",
    "        \n",
    "        #--------Third CNN block----------------\n",
    "        self.b3Conv1_0 = nn.Conv2d(512, 1024, \n",
    "                                 kernel_size=(1,1), stride=(1,1), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b3bn1_0 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        self.b3Conv2 = nn.Conv2d(1024, 1024, \n",
    "                                 kernel_size=(1,2), stride=(2,2), \n",
    "                                 padding=(0,1), bias=False, dilation= (1,2))\n",
    "        self.b3bn2 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        self.b3Conv3 = nn.Conv2d(1024, 1024, \n",
    "                                 kernel_size=(1,1), stride=(1,1), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b3bn3 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        self.b3Conv1_1 = nn.Conv2d(512, 1024, \n",
    "                                 kernel_size=(1,1), stride=(2,2), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b3bn1_1 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        #Add third basicblock (in matlb i = 2:6)\n",
    "        self.layer3 =  self.__make_layer(input_channel=1024, intermediate_channel=256, \n",
    "                                     num_blocks=5)\n",
    "        \n",
    "        #------------------\n",
    "        self.b4Conv1_0 = nn.Conv2d(1024, 2048, \n",
    "                                 kernel_size=(1,1), stride=(1,1), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b4bn1_0 = nn.BatchNorm2d(2048)\n",
    "        \n",
    "        self.b4Conv2 = nn.Conv2d(2048, 2048, \n",
    "                                 kernel_size=(1,2), stride=(1,1), \n",
    "                                 padding=(0,1), bias=False, dilation= (1,2))\n",
    "        self.b4bn2 = nn.BatchNorm2d(2048)\n",
    "        \n",
    "        self.b4Conv3 = nn.Conv2d(2048, 2048, \n",
    "                                 kernel_size=(1,1), stride=(1,1), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b4bn3 = nn.BatchNorm2d(2048)\n",
    "        \n",
    "        self.b4Conv1_1 = nn.Conv2d(1024, 2048, \n",
    "                                 kernel_size=(1,1), stride=(1,1), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b4bn1_1 = nn.BatchNorm2d(2048)\n",
    "        \n",
    "        #------\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc1 = nn.Linear(2048,2048)\n",
    "        self.fc1_bn = nn.BatchNorm1d(2048)\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        \n",
    "    def __load_word2vec(self, path):\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "    \n",
    "    def create_emb_layer(self, weights_matrix, non_trainable=False):\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape\n",
    "        emb_layer = torch.nn.Embedding(num_embeddings, embedding_dim)\n",
    "        emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "        if non_trainable:\n",
    "            emb_layer.weight.requires_grad = False\n",
    "\n",
    "        return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "       \n",
    "    def __make_layer(self, input_channel, intermediate_channel, num_blocks):\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            layers.append(BasicBlockText(input_channel, intermediate_channel))\n",
    "        return nn.Sequential(*layers)  \n",
    "\n",
    "    def forward(self, x): #input x: (N, 32)\n",
    "        x = self.emb_layer(x)# output-> (4,32,300)\n",
    "        x = x.permute(0,2,1).unsqueeze(2)\n",
    "        #--------------------\n",
    "        identity = x\n",
    "        out = self.relu(self.b1bn1_0(self.b1Conv1_0(x)))\n",
    "        out = self.relu(self.b1bn2(self.b1Conv2(out)))\n",
    "        out = self.b1bn3(self.b1Conv3(out))\n",
    "        #print(out.shape)\n",
    "        out2 = self.b1bn1_1(self.b1Conv1_1(identity))\n",
    "        #print(out2.shape)\n",
    "        out2 +=out\n",
    "        out2 = self.relu(out2) # (N, 256, 1,32)\n",
    "        #Add 2 basicblock\n",
    "        out2 = self.layer1(out2) # (N,256,1,32)\n",
    "        #------------------\n",
    "        identity = out2\n",
    "        out3 = self.relu(self.b2bn1_0(self.b2Conv1_0(out2)))\n",
    "        out3 = self.relu(self.b2bn2(self.b2Conv2(out3)))\n",
    "        out3 = self.b2bn3(self.b2Conv3(out3))\n",
    "        \n",
    "        out4 = self.b2bn1_1(self.b2Conv1_1(identity))\n",
    "        out4 += out3\n",
    "        out4 = self.relu(out4)\n",
    "        #Add 3 basicblocks\n",
    "        out4 = self.layer2(out4)## (N,512,1,16)\n",
    "        #-------------------------------\n",
    "        identity = out4\n",
    "        out5 = self.relu(self.b3bn1_0(self.b3Conv1_0(out4)))\n",
    "        out5 = self.relu(self.b3bn2(self.b3Conv2(out5)))\n",
    "        out5 = self.b3bn3(self.b3Conv3(out5))\n",
    "        \n",
    "        out6 = self.b3bn1_1(self.b3Conv1_1(identity))\n",
    "        out6 += out5\n",
    "        out6 = self.relu(out6)\n",
    "        #Add 5 basicblocks\n",
    "        out6 = self.layer3(out6)## (N,1024,1,8)\n",
    "        #---------------------------------------\n",
    "        identity = out6\n",
    "        out7 = self.relu(self.b4bn1_0(self.b4Conv1_0(out6)))\n",
    "        out7 = self.relu(self.b4bn2(self.b4Conv2(out7)))\n",
    "        out7 = self.b4bn3(self.b4Conv3(out7))\n",
    "        \n",
    "        out8 = self.b4bn1_1(self.b4Conv1_1(identity))\n",
    "        out8 += out7\n",
    "        out8 = self.relu(out8)\n",
    "        #-------------\n",
    "        out8 = self.avgpool(out8)\n",
    "        out8 = torch.flatten(out8,1)\n",
    "        out8 = self.dropout(self.relu(self.fc1_bn(self.fc1(out8))))\n",
    "        return out8\n",
    "        \n",
    "##Test TextCNN\n",
    "# net2 = textCNN(300, '../../../Downloads/GoogleNews-vectors-negative300.bin')\n",
    "# x2 = torch.rand(2,300,1,32)\n",
    "# y2 = net2(x2)\n",
    "# print(y2.shape) # (N,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement weight sharing class\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, output, stageI=True):\n",
    "        super(Model, self).__init__()\n",
    "        self.weights = torch.rand((output, 2048)).to(device) #out_features, in_features\n",
    "        self.imageCNN = ImageCNN(stageI)\n",
    "        self.textCNN =  textCNN(300)\n",
    "        \n",
    "    def forward(self, img, txt):\n",
    "        img_out = self.imageCNN(img) # (N,2048)\n",
    "        txt_out = self.textCNN(txt) # (N,2048)\n",
    "        return F.linear(img_out, self.weights), F.linear(txt_out, self.weights)\n",
    "\n",
    "##Test whole model\n",
    "# net = Model()\n",
    "# img = torch.rand((2,3,224,224))\n",
    "# txt = torch.rand((2,300,1,32))\n",
    "# fc_img, fc_txt = net(img, txt)\n",
    "# print(fc_img.shape) #(N,113287)\n",
    "# print(fc_txt.shape) #(N,113287)\n",
    "\n",
    "#define net\n",
    "net = Model(output=113287).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer\n",
    "optim = optim.Adam(net.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([35585, 57501, 22780, 54645])\n",
      "Epoch 0 loss at 0 iteration: 83.27410888671875\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([57239, 67748, 81313, 42985])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([ 45831, 106536,  95553,  19594])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([ 1953, 63891, 59227, 86525])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([ 75057,  36998,  59745, 101981])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([ 82328,  68578,  63715, 110933])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([57622, 99999, 74712,  4696])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([ 66289,  34882, 111510, 111927])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([34226, 55159, 47663, 12227])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([85977, 60351, 39941, 66207])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([ 50494,  78235,   2195, 101274])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([ 2855, 95278, 98473, 32463])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([49559, 36325, 23532, 98963])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([46138, 82076, 99632, 47232])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([99380, 91612,   246, 32830])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([83769, 60145, 96117, 81643])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([57191, 42936, 14918, 67036])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([69568, 95161, 65621, 92645])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([ 90422,  96246,  89825, 105276])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([ 4876, 12362, 71751, 24905])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([44163, 19689, 49592, 39506])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([ 64174,    655,  11471, 107871])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([ 10015,  34961,  93652, 111620])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([27732, 68325, 65235, 87122])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([107582,  23046, 106672,  84070])\n",
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 32]) tensor([92469, 22977, 39411, 74647])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-f581a4e28a29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainLoader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#-> test successful\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misgpu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\mlenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\mlenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-6517f4da007b>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'val2014/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'file_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mimg_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training\n",
    "net.train()\n",
    "epochs = 250\n",
    "loss = 0.0\n",
    "for e in range(epochs):\n",
    "    for i, (img, txt, label) in enumerate(trainLoader):\n",
    "        print(img.shape, txt.shape, label) #-> test successful\n",
    "        if isgpu:\n",
    "            img = img.to(device)\n",
    "            txt = txt.to(device)\n",
    "            label = label.to(device)\n",
    "        img_out, txt_out = net(img, txt)\n",
    "        #print(img_out.shape, txt_out.shape)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        imageLoss = criteria(img_out, label)\n",
    "        textLoss = criteria(txt_out, label)\n",
    "        loss =  imageLoss + textLoss\n",
    "        \n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        writer.add_scalar('data/imageLoss', imageLoss.item(), i)\n",
    "        writer.add_scalar('data/textLoss', textLoss.item(), i)\n",
    "        if (i%1000 ==0):\n",
    "            print('Epoch {} loss at {} iteration: {}'.format(e, i, loss.item()))\n",
    "#         if (i%200):\n",
    "#             net.eval()\n",
    "#             eval(net)\n",
    "#             net.train()\n",
    "        \n",
    "# export scalar data to JSON for external processing\n",
    "writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "writer.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((4,32,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.permute(0,2,1).unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['annotations'], test_dataset['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "82783+40504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
