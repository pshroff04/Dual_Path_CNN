{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2967f535f10>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.random.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset and preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "class ImageCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ImageCNN, self).__init__()\n",
    "        re = resnet50(pretrained=True)\n",
    "        #remove the last classification layer\n",
    "        self.resnet = nn.Sequential(*list(re.children())[:-1])\n",
    "        self.fc = nn.Linear(2048, 2048)\n",
    "        self.bn = nn.BatchNorm1d(2048)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropOut = nn.Dropout(0.8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x) # (N,2048,1,1)\n",
    "        x = torch.flatten(x,1) # (N,2048)\n",
    "        x = self.relu(self.bn(self.fc(x)))\n",
    "        x = self.relu(self.bn(self.fc(x))) # As per their MATLAB implementation\n",
    "        return self.dropOut(x)\n",
    "\n",
    "##Test Model -> Input (N,3,224,224) and Output (N,2048)\n",
    "# net1 = ImageCNN()\n",
    "# x1= torch.rand((2,3,224,224))\n",
    "# y = net1(x)\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#textCNN -> The input should be the output of word2vec (N,300,32,1)\n",
    "class BasicBlockText(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channel, intermediate_channel):\n",
    "        super(BasicBlockText, self).__init__()\n",
    "        self.bbConv1 = nn.Conv2d(input_channel, intermediate_channel, \n",
    "                                 kernel_size=(1,1), stride=(1,1), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.bbBatchNorm1 = nn.BatchNorm2d(intermediate_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.bbConv2 = nn.Conv2d(intermediate_channel, intermediate_channel, \n",
    "                                 kernel_size=(1,2), stride=(1,1), \n",
    "                                 padding=(0,1),bias=False, dilation = (1,2))\n",
    "        self.bbBatchNorm2 = nn.BatchNorm2d(intermediate_channel)\n",
    "        \n",
    "        self.bbConv3 = nn.Conv2d(intermediate_channel, input_channel, kernel_size=(1,1), stride=(1,1), padding=(0,0), bias=False)\n",
    "        self.bbBatchNorm3 = nn.BatchNorm2d(input_channel)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity  = x\n",
    "        out = self.relu(self.bbBatchNorm1(self.bbConv1(x)))\n",
    "        out = self.relu(self.bbBatchNorm2(self.bbConv2(out)))\n",
    "        out = self.bbBatchNorm3(self.bbConv3(out))\n",
    "        \n",
    "        out += identity\n",
    "        return self.relu(out)\n",
    "\n",
    "\n",
    "        \n",
    "class textCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channel): #300\n",
    "        super(textCNN, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        #--------First CNN block----------------\n",
    "        self.b1Conv1_0 = nn.Conv2d(input_channel, 128, \n",
    "                                 kernel_size=(1,1), stride=(1,1), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b1bn1_0 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.b1Conv2 = nn.Conv2d(128, 128, \n",
    "                                 kernel_size=(1,2), stride=(1,1), \n",
    "                                 padding=(0,1), bias=False, dilation=(1,2))\n",
    "        self.b1bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.b1Conv3 = nn.Conv2d(128, 256, \n",
    "                                 kernel_size=(1,1), stride=(1,1), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b1bn3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        #input here too\n",
    "        self.b1Conv1_1 = nn.Conv2d(input_channel, 256,\n",
    "                               kernel_size=(1,1), stride=(1,1), \n",
    "                               padding=(0,0), bias=False)\n",
    "        self.b1bn1_1 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        #Adding first basicblock (in matlab code i=2:3)\n",
    "        self.layer1  = self.__make_layer(input_channel=256, intermediate_channel=64, \n",
    "                                     num_blocks=2)\n",
    "        \n",
    "        #--------Second CNN block----------------\n",
    "        self.b2Conv1_0 = nn.Conv2d(256, 512, \n",
    "                                 kernel_size=(1,1), stride=(1,1), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b2bn1_0 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.b2Conv2 = nn.Conv2d(512, 512, \n",
    "                                 kernel_size=(1,2), stride=(2,2), \n",
    "                                 padding=(0,1), bias=False, dilation= (1,2))\n",
    "        self.b2bn2 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.b2Conv3 = nn.Conv2d(512, 512, \n",
    "                                 kernel_size=(1,1), stride=(1,1), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b2bn3 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.b2Conv1_1 = nn.Conv2d(256, 512, \n",
    "                                 kernel_size=(1,1), stride=(2,2), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b2bn1_1 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        #Add second basicblock (in matlb i = 2:4)\n",
    "        self.layer2 =  self.__make_layer(input_channel=512, intermediate_channel=128, \n",
    "                                     num_blocks=3)\n",
    "        \n",
    "        #--------Third CNN block----------------\n",
    "        self.b3Conv1_0 = nn.Conv2d(512, 1024, \n",
    "                                 kernel_size=(1,1), stride=(1,1), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b3bn1_0 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        self.b3Conv2 = nn.Conv2d(1024, 1024, \n",
    "                                 kernel_size=(1,2), stride=(2,2), \n",
    "                                 padding=(0,1), bias=False, dilation= (1,2))\n",
    "        self.b3bn2 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        self.b3Conv3 = nn.Conv2d(1024, 1024, \n",
    "                                 kernel_size=(1,1), stride=(1,1), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b3bn3 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        self.b3Conv1_1 = nn.Conv2d(512, 1024, \n",
    "                                 kernel_size=(1,1), stride=(2,2), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b3bn1_1 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        #Add third basicblock (in matlb i = 2:6)\n",
    "        self.layer3 =  self.__make_layer(input_channel=1024, intermediate_channel=256, \n",
    "                                     num_blocks=5)\n",
    "        \n",
    "        #------------------\n",
    "        self.b4Conv1_0 = nn.Conv2d(1024, 2048, \n",
    "                                 kernel_size=(1,1), stride=(1,1), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b4bn1_0 = nn.BatchNorm2d(2048)\n",
    "        \n",
    "        self.b4Conv2 = nn.Conv2d(2048, 2048, \n",
    "                                 kernel_size=(1,2), stride=(1,1), \n",
    "                                 padding=(0,1), bias=False, dilation= (1,2))\n",
    "        self.b4bn2 = nn.BatchNorm2d(2048)\n",
    "        \n",
    "        self.b4Conv3 = nn.Conv2d(2048, 2048, \n",
    "                                 kernel_size=(1,1), stride=(1,1), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b4bn3 = nn.BatchNorm2d(2048)\n",
    "        \n",
    "        self.b4Conv1_1 = nn.Conv2d(1024, 2048, \n",
    "                                 kernel_size=(1,1), stride=(1,1), \n",
    "                                 padding=(0,0), bias=False)\n",
    "        self.b4bn1_1 = nn.BatchNorm2d(2048)\n",
    "        \n",
    "        #------\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc1 = nn.Linear(2048,2048)\n",
    "        self.fc1_bn = nn.BatchNorm1d(2048)\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        \n",
    "        \n",
    "    def __make_layer(self, input_channel, intermediate_channel, num_blocks):\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            layers.append(BasicBlockText(input_channel, intermediate_channel))\n",
    "        return nn.Sequential(*layers)  \n",
    "\n",
    "    def forward(self, x): #input x: (N, 256, 1, 32)\n",
    "        #--------------------\n",
    "        identity = x\n",
    "        out = self.relu(self.b1bn1_0(self.b1Conv1_0(x)))\n",
    "        out = self.relu(self.b1bn2(self.b1Conv2(out)))\n",
    "        out = self.b1bn3(self.b1Conv3(out))\n",
    "        #print(out.shape)\n",
    "        out2 = self.b1bn1_1(self.b1Conv1_1(identity))\n",
    "        #print(out2.shape)\n",
    "        out2 +=out\n",
    "        out2 = self.relu(out2) # (N, 256, 1,32)\n",
    "        #Add 2 basicblock\n",
    "        out2 = self.layer1(out2) # (N,256,1,32)\n",
    "        #------------------\n",
    "        identity = out2\n",
    "        out3 = self.relu(self.b2bn1_0(self.b2Conv1_0(out2)))\n",
    "        out3 = self.relu(self.b2bn2(self.b2Conv2(out3)))\n",
    "        out3 = self.b2bn3(self.b2Conv3(out3))\n",
    "        \n",
    "        out4 = self.b2bn1_1(self.b2Conv1_1(identity))\n",
    "        out4 += out3\n",
    "        out4 = self.relu(out4)\n",
    "        #Add 3 basicblocks\n",
    "        out4 = self.layer2(out4)## (N,512,1,16)\n",
    "        #-------------------------------\n",
    "        identity = out4\n",
    "        out5 = self.relu(self.b3bn1_0(self.b3Conv1_0(out4)))\n",
    "        out5 = self.relu(self.b3bn2(self.b3Conv2(out5)))\n",
    "        out5 = self.b3bn3(self.b3Conv3(out5))\n",
    "        \n",
    "        out6 = self.b3bn1_1(self.b3Conv1_1(identity))\n",
    "        out6 += out5\n",
    "        out6 = self.relu(out6)\n",
    "        #Add 5 basicblocks\n",
    "        out6 = self.layer3(out6)## (N,1024,1,8)\n",
    "        #---------------------------------------\n",
    "        identity = out6\n",
    "        out7 = self.relu(self.b4bn1_0(self.b4Conv1_0(out6)))\n",
    "        out7 = self.relu(self.b4bn2(self.b4Conv2(out7)))\n",
    "        out7 = self.b4bn3(self.b4Conv3(out7))\n",
    "        \n",
    "        out8 = self.b4bn1_1(self.b4Conv1_1(identity))\n",
    "        out8 += out7\n",
    "        out8 = self.relu(out8)\n",
    "        #-------------\n",
    "        out8 = self.avgpool(out8)\n",
    "        out8 = torch.flatten(out8,1)\n",
    "        out8 = self.dropout(self.relu(self.fc1_bn(self.fc1(out8))))\n",
    "        return out8\n",
    "        \n",
    "##Test TextCNN\n",
    "# net2 = textCNN(300)\n",
    "# x2 = torch.rand(2,300,1,32)\n",
    "# y2 = net2(x2)\n",
    "# print(y2.shape) # (N,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2048])\n",
      "torch.Size([2, 113287])\n",
      "torch.Size([2, 113287])\n"
     ]
    }
   ],
   "source": [
    "#implement weight sharing class\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.weights = torch.rand((113287, 2048)) #out_features, in_features\n",
    "        self.imageCNN = ImageCNN()\n",
    "        self.textCNN =  textCNN(300)\n",
    "        \n",
    "    def forward(self, img, txt):\n",
    "        img_out = self.imageCNN(img) # (N,2048)\n",
    "        txt_out = self.textCNN(txt) # (N,2048)\n",
    "        return F.linear(img_out, self.weights), F.linear(txt_out, self.weights)\n",
    "\n",
    "##Test whole model\n",
    "# net = Model()\n",
    "# img = torch.rand((2,3,224,224))\n",
    "# txt = torch.rand((2,300,1,32))\n",
    "# fc_img, fc_txt = net(img, txt)\n",
    "# print(fc_img.shape) #(N,113287)\n",
    "# print(fc_txt.shape) #(N,113287)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix the weights of resnet during stage I training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
